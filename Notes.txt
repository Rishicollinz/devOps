8/3/24
======
Multipass docs:https://multipass.run/docs/how-to-guides
Installing multipass:
=====================
1.check if snap is available: cmd: snap

2.Install multipass : sudo snap install multipass

3.cmd:ls -l /var/snap/multipass/common/multipass_socket
    output:srw-rw---- 1 root sudo 0 Mar  8 10:39 /var/snap/multipass/common/multipass_socket
    ->-l gives detailed information.
    ->srw-rw---- meaning is s indicates socket file rw means read/write permission for root and null for others.

4.groups | grep sudo
    Explanation: groups lists all the groups of current user. grep sudo will check for sudo in previous command's output and highlight it.
    output:rishikeshb adm cdrom sudo dip plugdev lpadmin lxd sambashare docker

5.snap info multipass - will give info about multipass.
    output:
    name:      multipass
    summary:   Instant Ubuntu VMs
    publisher: Canonical**
    store-url: https://snapcraft.io/multipass
    contact:   https://github.com/canonical/multipass/issues/new
    license:   GPL-3.0
    description: |
    Multipass is a tool to launch and manage VMs on Windows, Mac and Linux that
    simulates a cloud environment with support for cloud-init. Get Ubuntu
    on-demand with clean integration to your IDE and version control on your
    native platform.
    
    Launch an instance (by default you get the current Ubuntu LTS)
    
        multipass launch --name foo
    
    Run commands in that instance, try running bash (logout or ctrl-d to quit)
    
        multipass exec foo -- lsb_release -a
    
    Pass a cloud-init metadata file to an instance on launch
    
        multipass launch -n bar --cloud-init cloud-config.yaml
    
    See your instances
    
        multipass list
    
    Stop and start instances
    
        multipass stop
        multipass start
    
    Get help
    
        multipass help
    commands:
    - multipass.gui
    - multipass
    services:
    multipass.multipassd: simple, enabled, active
    snap-id:      mA11087v6dR3IEcQLgICQVjuvhUUBUKM
    tracking:     latest/stable
    refresh-date: today at 10:39 IST
    channels:
    latest/stable:    1.13.1                    2024-02-12 (11732) 122MB -
    latest/candidate: 1.13.1                    2024-03-07 (11967) 122MB -
    latest/beta:      1.13.1                    2024-02-12 (11732) 122MB -
    latest/edge:      1.14.0-dev.1536+g2bf881fd 2024-03-05 (11946)  67MB -
    installed:          1.13.1                               (11732) 122MB -

->cmd: multipass for all the commands available in multipass.

Uninstall multipass:
=====================
->snap remove multipass

Creating an instance:
=====================
1)1st way: Multipass GUI
->open multipass app and click multipass icon on right top tray and click on shell. It creates a new instance named primary with 1gb ram and 1 cpu and 5 gb of disk space.

2)2nd way: multipass launch
->It also creates a instance with random name with 1gb ram,1 cpu and 5 gb of storage.
ex:multipass launch
Launched: right-verdin

    Ex:multipass info right-verdin
    Name:           right-verdin
    State:          Running
    Snapshots:      0
    IPv4:           10.94.51.18
    Release:        Ubuntu 22.04.4 LTS
    Image hash:     fa2146bb04e5 (Ubuntu 22.04 LTS)
    CPU(s):         1
    Load:           0.37 0.18 0.07
    Disk usage:     1.6GiB out of 4.8GiB
    Memory usage:   182.4MiB out of 951.9MiB
    Mounts:         --

3) Creating an instance with specific image with everything custom

cmd:multipass find
    ->lists all the available image

cmd: multipass launch <alias_name/versionNO> --name <custom_name> --cpus <No.0f.cpus(4)> --memory <8G> --disk <size in G> primary(optional);

->multipass delete <instance_name> - to delete an instance.
->multipass purge <instance_name> - to remove from the list.

Modify an instance:
===================

->After creation of the instance, we can modify them by two ways:
    ->set the CPU, RAM or disk of the instance.
    ->set the instance as primary.

SSH:= secure socket shell.
====

->ssh protocol is a method for securely sending commands to a computer over an unsecured network.
->ssh uses cryptography to authenticate and encrypt connections between devices.

->TCP/IP - https://www.cloudflare.com/en-gb/learning/ddos/glossary/tcp-ip/

->How IP address work -
IPV4 - 32 bit.
IPV6 - 128 bit.

static IP:
These ip don't change over time.

dynamic IP:
->Assigning a temporary ip for a time by ISP from a shared pool of ip's.

what is ssh?
ans:
https://www.cloudflare.com/en-gb/learning/access-management/what-is-ssh/

passwd for ubuntu18LTS = ubuntu18 
passwd for ubuntu22LTS = ubuntu22 

How to connect two linux machines using ssh keys:
============================================
https://automateinfra.com/2021/02/25/how-to-connect-two-linux-machines-using-ssh-keys/


ubuntu 18 keypair gen:
=======================
ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/home/ubuntu/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/ubuntu/.ssh/id_rsa.
Your public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:q73NxLYxuSuXAXznKTCWnSvVzeLzTgKKJuU+6g5+rxw ubuntu@ubuntu18LTS
The key's randomart image is:
+---[RSA 2048]----+
|                 |
|                 |
|       . o o o   |
|        B = + o  |
|      ..S*.= o   |
|     o ..++o=    |
|  . E + o.B+.o.  |
| . o.=.o.=o= o.  |
|  .+*++.o+*. ..  |
+----[SHA256]-----+

ubuntu 22 keypair gen:
======================
ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/home/ubuntu/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/ubuntu/.ssh/id_rsa
Your public key has been saved in /home/ubuntu/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:rD/O91ZyKBSpSlnZiGG+fJ9Buwai25NOfPiIVEk2WPE ubuntu@ubuntu22LTS
The key's randomart image is:
+---[RSA 3072]----+
|      =+ + .     |
|     =..+ +      |
|    . =oE...     |
|     +o=....     |
|     .*.S.o  .   |
|     +.= o.+o o  |
|    o =.. =. +   |
|   . =o*... .    |
|    o.+o=o o.    |
+----[SHA256]-----+

SSH:
->ssh is used to access another remote machine in local machine.

->For ssh to work,
    1->Generate ssh-keygen for the source.
    2->place the public-key of the source in the destination's authorized key
    ->Access by this cmd: ssh ubuntu<user-name>@ip

SSH port forward/thunneling:4

    1.Follow the first 2 steps from the above.
    2.In the receiver's side, use this cmd: ssh -L 3001:localhost:80 ubuntu@188.17.0.5

scp: copies file between two networks

Method 1:scp srinath@192.168.1.26:/home/srinath/my_practice/JavaScript.pdf .
scp <Source@IP>:<source_folder> <destination_folder>

Method 2:scp array.js rishikeshb@192.168.1.63:.
scp <fromhere_file> destination@IP:.


Git:
===

->Git config:
=============

=>These configurations can be applied globally, per user, or per repository basis.

To check config details:-> git config --list --show-origin

To set username and email:
$ git config --global user.name "John Doe"
$ git config --global user.email johndoe@example.com

To set main as the default branch name do:

$ git config --global init.defaultBranch main

getting help:
============

cmd:
$git <verb> --help = for long manual page.
$git <verb> -h= for short helping hand.


ways to start git:
==================
1. git clone from existing repo= git clone <url> <destination if any>
2. initialization of folder.=git init .

Recording changes to the repository:
======================================

->Tracked files are files git knows about.
->Untracked files are files that git doesn't know about.
e.g: A folder with no vcs.

git status => to check the state of the files in the git.

skipping the staging area:
==========================
->git commit -a -m "the staging is skipped"

Removing files:
===============

=>git rm file - It will remove the file from the repo as well as working directory. and the changes will be in staged area, we have to commit it.

=>If you modified the file or had already added it to the staging area, you must force the removal with the -f option. This is a safety feature to prevent accidental removal of data that hasn’t yet been recorded in a snapshot and that can’t be recovered from Git.

To remove only from the repo but not locally:

git rm --cached Readme

Moving Files:
==============
Unlike many other VCSs, Git doesn’t explicitly track file movement. If you rename a file in Git, no metadata is stored in Git that tells it you renamed the file. However, Git is pretty smart about figuring that out after the fact — we’ll deal with detecting file movement a bit later.

$ git mv file_from file_to

viewing the commit history:
===========================


Git Task:
========
two common repo, diff branch.
->ssh key authenticate
history read/write, hard reset,merge conflict, merge, workflow setup.

Bit Bucket:
===========

1.Workspace.
2.Creating workspace.
3.Updating the workspace.
4.Inviting people to workspace.
5.Adding a group to workspace.
6.Adding user access and permissions for repository.
7.

Cloning an repo:
===============
ssh:
===
1.Add your pubkey into repo access key place.
2.Then clone using ssh.

http:
=====
1.Install git credential manager:
https://github.com/git-ecosystem/git-credential-manager/blob/release/docs/install.md

//create a app password and set it to helper cache
git config credential.helper cache


//don't use this 
2.set up credential store:
git config --global credential.credentialStore gpg

3.set up a pass store:
pass init <gpg-id> // gpg-id(random-my id is:)=25012003

4.gpg --full-generate-key
    gpg (GnuPG) 2.2.27; Copyright (C) 2021 Free Software Foundation, Inc.
    This is free software: you are free to change and redistribute it.
    There is NO WARRANTY, to the extent permitted by law.

    Please select what kind of key you want:
    (1) RSA and RSA (default)
    (2) DSA and Elgamal
    (3) DSA (sign only)
    (4) RSA (sign only)
    (14) Existing key from card
    Your selection? 1
    RSA keys may be between 1024 and 4096 bits long.
    What keysize do you want? (3072) 
    Requested keysize is 3072 bits
    Please specify how long the key should be valid.
            0 = key does not expire
        <n>  = key expires in n days
        <n>w = key expires in n weeks
        <n>m = key expires in n months
        <n>y = key expires in n years
    Key is valid for? (0) 
    Key does not expire at all
    Is this correct? (y/N) y

    GnuPG needs to construct a user ID to identify your key.

    Real name: Rishikesh
    Email address: rishikesh.b@codingmart.com
    Comment: bitbuckethttpclone
    You selected this USER-ID:
        "Rishikesh (bitbuckethttpclone) <rishikesh.b@codingmart.com>"

    Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O
    We need to generate a lot of random bytes. It is a good idea to perform
    some other action (type on the keyboard, move the mouse, utilize the
    disks) during the prime generation; this gives the random number
    generator a better chance to gain enough entropy.
    We need to generate a lot of random bytes. It is a good idea to perform
    some other action (type on the keyboard, move the mouse, utilize the
    disks) during the prime generation; this gives the random number
    generator a better chance to gain enough entropy.
    gpg: key BEFEC50FFBB08338 marked as ultimately trusted
    gpg: directory '/home/rishikeshb/.gnupg/openpgp-revocs.d' created
    gpg: revocation certificate stored as '/home/rishikeshb/.gnupg/openpgp-revocs.d/537EACF965CB725CA54815C1BEFEC50FFBB08338.rev'
    public and secret key created and signed.

    pub   rsa3072 2024-03-12 [SC]
        537EACF965CB725CA54815C1BEFEC50FFBB08338
    uid                      Rishikesh (bitbuckethttpclone) <rishikesh.b@codingmart.com>
    sub   rsa3072 2024-03-12 [E]
    passphrase:wantedbyme

    5.gpg --export --armor rishikesh.b@codingmart.com
    -----BEGIN PGP PUBLIC KEY BLOCK-----

    mQGNBGXwHYEBDADVSfKcKgsHHTFGu6BmzTzgI73Nk91fN6Lm7mne0+AS5CEdC6yh
    6eVhmszVYVzeF/o/8ywZ6lwa7hwRPdY93IwdBPgJazJB6qeiA2bBOFjgtmdZx1yc
    dfkxCVQCtJ1vzJtOKTGMX3Y0ydU2/6ytEZJcPBrF6kuse2lgvzKOvPudtarckvkZ
    u7HwpbeHHRpcq7zp0vyi0E9sUNIgBGNCVA++ShFywmEDCSHG9KBf4wuQoukN5rdH
    XH+jYFp18UFx+4qrHTHnRP+IzyXMYrBgniZtWmvkwyswvpfE5mVUjZ2QJ6mkEb1T
    9cLXcPQ80OhewWb8F0LPdl4FZHfpSbIunuivMWA2cZiLT/4OZzvXRNAvYyTCTHbw
    zu76baiGc6AXXS88R/PNrw05fyMEsN2dVbe8KWS2cKNdh/gbG429N2x81JE0XGJM
    63A8nJzfZA4xrKZ8CnA1t0DuqBrO79fs4yPCEQlnHtaGRG/t7U4gjorA8Sbt2rzZ
    xHSVv/FMF9K58icAEQEAAbQ7UmlzaGlrZXNoIChiaXRidWNrZXRodHRwY2xvbmUp
    IDxyaXNoaWtlc2guYkBjb2RpbmdtYXJ0LmNvbT6JAc4EEwEKADgWIQRTfqz5Zcty
    XKVIFcG+/sUP+7CDOAUCZfAdgQIbAwULCQgHAgYVCgkICwIEFgIDAQIeAQIXgAAK
    CRC+/sUP+7CDOCYDDAClfSPj4SRvWQ4WNR1cYu4u2pfVaKhOkpaT2fQGRt0EtxQo
    t9FN8yDauuEdU1YRc4D9RH2gXzWEn/xLc/kJ0o+YpXYcleI+zMcv+5uQTvx7TMCklpf
    JPRVDeGmFUbQHtT1N8AbMJYk/bdU74cSVf0ONfw4DplJ5riQ6qd2zfTFM9KX+Dft
    o27lJVnrtiWhH1/NrROTzcEn+pax9R+fsYzOKIQmAZjxCXiLja4SPpiSLazRLNPn
    CmdbnicwXfGSeMZapWqMMHswUmkNuzTpvc2FE0pK55KFfEsFLX6eZEuSVv5y/ckQ
    qyioa2BR77Z8GMHeQo44OjcxCYc2wpQ8ozQ4a7HniPrKsjCMmvbmE67UP9i6hPmC
    9y23jTMkEN407RfsZSIt7pfa2rjTmfnlzDhBoEHolodNY+KeSFrf/dtodDe8SvGp
    KRnXx6y4L9R6x23okj62QTDALxUC1AY+OcfpJhp9pbnIKO6u/qPSrSF1OlzpBQxN
    RsAB23I5J2gRC+x1z5C5AY0EZfAdgQEMALVExOFVxOc4nr4NVEcsMVqQVWsy47i2
    UyDTwbt9daLKvxhaViC89umVUetnC+K3V+yk1b7Gds93GDa3Rh/KQyZDCvcoIh/2
    h68AdgGOzqh8GSMpN1CP675jDNvo2XfRGqP7oMBI6m+B9U4AzGGt3oAyt7n7+gTf
    0fdMlYk8CDbb75ocW7Q7a/w3lNM2v/n2i8ZfYwFxzWN64ogt9wjPRLCjKbtWoOa9
    qjgFqfaJr55W2Y3dXVBDSr24AN6W6LAO45FISIRsoKK/SB7e8OAvlC5twtzKsdqs
    pyTfIn80SUc8A4uczOqqtVqWU/bo3dH59lRkMZXfAFulq6nYlToJYy/WO7ssFCdq
    qzV8+tbJDm/aSzX6+TfGDD94qwOaOs8OTN8u9aWvEa8uwKEvofbEbHoNL/T3B0js
    shAYfNqMDFszZ/z7BFFYOJerQW1TPFMFcW7tunEB7wK7viWuH8JtZ06oa8xbsHQQ
    x6qDDdLI9pZTB31o1D4CuXevHTGhXRZgXwARAQABiQG2BBgBCgAgFiEEU36s+WXL
    clylSBXBvv7FD/uwgzgFAmXwHYECGwwACgkQvv7FD/uwgzj7TQv+MNsKjmx3b9s4
    Hqr2l8kDFduWAh5/7hCfhulQ3tXAOaz1zskDeYdjZoYzOdEjiPPGqJUbsxLWTRSo
    DqGjHawiCPy7iRZFsTAqQfd8AmOFKVYJRWn85Orx7L/MHppak45n0c2Gy4wDfQcT
    TWcdUhZkPRt7US7Xi3QSwHUCmjyZoMixZYblXd0MUYIpf3uH76qYNovnAVO8B+95
    +SBqOuBnle4F5iFJah0LsTb9IMY/RL9B++7nEgTydws5PcVUNEYyhLCSCA5XRfKs
    xZQmN6aKkgROHhCI/PChi09uL6vnF7NRFzdKy1z94pSQ7362BssWhz0gMz5E1cSP
    O9DUw/x3yKeDUNyRltHM6qWC9HdnuC/8H9SJLTIuztGdenylu/JgyUGcPmAYFZTe
    5PAasRWvWv+Rwy7YHEB/g9Ju5/JfOyBZRWzAHrc+MoeN9zRJHStEOJhRWd/Q4b8T
    g+s8Mrx5cG/dHh8idOPP8kRiu0iELd+TSaM5lLO681Onnk0zWpWX
    =Rlor
    -----END PGP PUBLIC KEY BLOCK-----

branch:https://www.w3docs.com/snippets/git/how-to-create-a-remote-branch-in-git.html#pushing-a-local-branch-to-remote-9

Git merge conflict:
git checkout main
Switched to branch 'main'
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$ git merge sribranch 
Updating ca2e19f..dd2889a
Fast-forward
 trial.txt | 1 +
 1 file changed, 1 insertion(+)
 create mode 100644 trial.txt
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$ git commit -m "trialm"
On branch main
Your branch is ahead of 'origin/main' by 2 commits.
  (use "git push" to publish your local commits)

nothing to commit, working tree clean
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$ git add .
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$ git commit -m "trialm"
On branch main
Your branch is ahead of 'origin/main' by 2 commits.
  (use "git push" to publish your local commits)

nothing to commit, working tree clean
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$ git push
To bitbucket.org:learn-cm-devops/mergeconflictrepo.git
 ! [rejected]        main -> main (fetch first)
error: failed to push some refs to 'bitbucket.org:learn-cm-devops/mergeconflictrepo.git'
hint: Updates were rejected because the remote contains work that you do
hint: not have locally. This is usually caused by another repository pushing
hint: to the same ref. You may want to first integrate the remote changes
hint: (e.g., 'git pull ...') before pushing again.
hint: See the 'Note about fast-forwards' in 'git push --help' for details.
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$ git config pull.rebase true
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$ git pull
remote: Enumerating objects: 9, done.
remote: Counting objects: 100% (9/9), done.
remote: Compressing objects: 100% (6/6), done.
remote: Total 7 (delta 2), reused 0 (delta 0), pack-reused 0
Unpacking objects: 100% (7/7), 627 bytes | 313.00 KiB/s, done.
From bitbucket.org:learn-cm-devops/mergeconflictrepo
   26a54dd..6a921fd  main        -> origin/main
 * [new branch]      rishiBranch -> origin/rishiBranch
Auto-merging hello.txt
CONFLICT (add/add): Merge conflict in hello.txt
error: could not apply ca2e19f... sri2
hint: Resolve all conflicts manually, mark them as resolved with
hint: "git add/rm <conflicted_files>", then run "git rebase --continue".
hint: You can instead skip this commit: run "git rebase --skip".
hint: To abort and get back to the state before "git rebase", run "git rebase --abort".
Could not apply ca2e19f... sri2
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$ git add hello.txt trial.txt
fatal: pathspec 'trial.txt' did not match any files
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$ git add hello.txt
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$ git rebase --continue
[detached HEAD 1e68dac] sri3
 1 file changed, 3 insertions(+), 1 deletion(-)
Successfully rebased and updated refs/heads/main.
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$ git commit -m "rebased"
On branch main
Your branch is ahead of 'origin/main' by 2 commits.
  (use "git push" to publish your local commits)

nothing to commit, working tree clean
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$ git add .
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$ git commit -m "rebased"
On branch main
Your branch is ahead of 'origin/main' by 2 commits.
  (use "git push" to publish your local commits)
dd
nothing to commit, working tree clean
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$ git push
Enumerating objects: 8, done.
Counting objects: 100% (8/8), done.
Delta compression using up to 8 threads
Compressing objects: 100% (4/4), done.
Writing objects: 100% (6/6), 570 bytes | 570.00 KiB/s, done.
Total 6 (delta 1), reused 0 (delta 0), pack-reused 0
To bitbucket.org:learn-cm-devops/mergeconflictrepo.git
   6a921fd..46390d1  main -> main
srinath@srinath-Latitude-3520:~/my_practice/merge/mergeconflictrepo$

13/3/24
branch restrictions:
==================

https://support.atlassian.com/bitbucket-cloud/docs/configure-a-projects-branch-restrictions/

1.Branch restrictions with branch conflict and resolve, reviewers.
2.git revert, reset --hard --soft --mixed.
3.Branch restrictions for push --force.

ssh protecting by firewall:
1.ufw package method: https://help.ubuntu.com/community/UFW
    ubuntu@ubuntu22LTS:~$ sudo apt-get install ufw
    Reading package lists... Done
    Building dependency tree... Done
    Reading state information... Done
    ufw is already the newest version (0.36.1-4ubuntu0.1).
    ufw set to manually installed.
    0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.
    ubuntu@ubuntu22LTS:~$ sudo ufw enable
    Command may disrupt existing ssh connections. Proceed with operation (y|n)? y
    Firewall is active and enabled on system startup
    ubuntu@ubuntu22LTS:~$ sudo ufw allow from 10.94.51.45
    Rule added
    ubuntu@ubuntu22LTS:~$ sudo ufw reload
    Firewall reloaded
    ubuntu@ubuntu22LTS:~$ sudo ufw status
    Status: active

    To                         Action      From
    --                         ------      ----
    Anywhere                   ALLOW       10.94.51.45     

Install archLinux in virtual box on ubuntu:
==============================================

1.Install virtual box:
======================

14/3/24 =nvim+nvchad:
=====================
nvchad = https://nvchad.com/docs/quickstart/install/
neovim = https://github.com/neovim/neovim/releases

homebrew installation:https://www.how2shout.com/linux/install-brew-on-ubuntu-22-04-lts-jammy-linux/

Docker:
=======

1.create a docker file
steps:
    FROM node:20
    WORKDIR /app
    COPY . .
    RUN npm i
    CMD ["npm", "run", "dev"]
    EXPOSE 5173
2.sudo systemctl start docker = start the docker engine otherwise open docker desktop.

3.Build an docker image:
cmd: docker build -t ex5img . (this img name should be lowercase)

4.Create a container:

15/3/24
Docker tutorial:
===============

=>Docker uses container to run a project in an isolated environment.

=>Normally, if a developer develops a project, they install several libs(i.e dependencies). so if someone who need to simulate the project, they have to install all the dependencies which is not fesible.

=>So in docker, there is a container which already has all the necessary dependencies, the users can just run the image without needing to install them.

Installing Docker:
==================

1.Install docker desktop or only engine on linux.

Docker Images:
=============

1.It is blueprint for containers.

    ->It contains,
        ->Runtime environment,
        ->App code.
        ->Any dependencies
        ->Extra configurations
        ->Commands.

    ->It is immutable meaning we can't edit the image after its creation.
    ->It contains everything we need to run our code.

Docker Container:
=================
->Runnable instance of an image.
->Completely isolated.

Docker Images:
=============

=>Images are made up from several "layers"

Image:
3.dependencies
2.Source code
1.Parent image

1.Parent image: this parent image can be node,python etc.we can pull the image from the docker hub and use it in our code.

Installing docker engine on 24.04:
https://linuxconfig.org/quick-docker-installation-on-ubuntu-24-04

1.dockerignore

->Layer caching:
================
->Every line in docker file is a layer.

->So on building an image, the image does all the process first time but it checks if there is a same layer already available upon available it is cached.

->so if a layer is new, it is pulled .

->If just a layer changes, the lines above that are cached, the lines below that are downloaded again.


Image pushing:
=============

1.create a account with docker hub.
2.create a repository.
3. syntax: docker push <your-user-name>/myapp
4.Login into to your docker hub.
    docker login -u YOUR-USER-NAME
5.But there should be a img locally in this format:
    <your-user-name>/imgname
    ->to do that 
        ->docker tag <old img name> <new img name(rishikeshb/myapp)>

6.docker push <your-username>/<img_name>

Run a image from hub:

1.docker run -dp 0.0.0.0:3000:3000 YOUR-USER-NAME/getting-started

volume:
=======

=>Generally when a code changes, then we have to rebuilt an image, then start container with it. 

=>Volume is like mapping the a folder in local to the folder in the container, so whenever a folder changes, the container's folder also changes.

=>we can use nodemon to restart the server.

set up process:
===============

1. Instead of npm run , in package.json add dev in script with nodemon.
2. use run "npm install -g nodemon" on dockerfile and npm run dev on it.
3. while run a container use -v flag with <localPath>:/app 

volume mount --- persist the db:
===============
1.create a new volume:
docker volume create <vol-name>

2.While running the container, use --mount type=volume,src=<vol-name>,target=/etc/todos <img-name>
->here target is inside the container.

3.we can delete the container and create a new one and mount the volume and the data will persist.


Bind Mount:
===========
=>volume mount is used when we need a persistent place to store our data.
=>A bind mount is another type of mount, which lets us to share the host's filesystem into a container.
=> --mount type=bind,src=<bind_folder_local>,target=<container-folder>
=> -v <localpath>:<targetPath>

docker compose:
==============

1.when we have to run multi container or multi images.

2.Create a docker-compose.yaml file 
like this:
    version: "3.8"
    services:
    myapp:
        build: ./myapp
        container_name: myapp-c 
        ports:
        - '3000:3001'
        volumes:
        - ./myapp:/app
        - /app/node_modules

3.run docker compose up

docker dev environment:
=======================
=requirements:
1.docker file
2.docker compose file.

method-1:
=>Create a image,container,compose file
=>docker compose run --rm -d --service-ports openjdkenv
=>for shell: docker exec -it <container_id> sh

Docker compose:
===============

=>compose v1 needs top level version in compose.yaml file. and it used docker-compose command.
=>compose v2 don't need version on top of compose.yaml file. and it uses docker compose command.

compose plugin vs compose standalone:
=====================================

=>
Docker network:
===============

=>docker network ls => will list all the current docker network.

1.Bridge network:
===================

->Default.
=>Driver=network type.
=>Each container in bridge network will have its own ip.
=>It will take the config file from the host (docker 0);
=>Each container will have virtual ethernet.

Problem with bridge network:
============================
=>It wont automatically expose the services(port) running on the container.
=>While running the container, we have to specify the -p flag for the port publish.

2.User-defined bridge:
======================
=>ip address show =>to see all the ip's in the machine.

->cmd : docker network create <name> =>to create a new network.

->cmd: while running the container, "--network <network_name>" to set a network to a container.

->cmd: bridge link => to see the container link to a network.

->we can connect two containers in the same network but we can't ping two containers in differnet network.

Adv:
===
1.It automatically dns the ip of the container in its network based on the name of the container.

3.Host network:
===============

cmd: docker run --network host

=>the container will attach to host network, no port forwarding,
just ip and port of the direct host.

=>No isolation here.

4.MacVLan:
==========
=>It connects the container like a vm to the network.
=>There is no docker between the container and machine.

cmd: sudo docker network create -d macvlan --subnet <home_ip> --gateway 10.7.1.3 -o parent=enp0s3 <network-name>

to run cont:
docker run --ip <we have to give ip>
=>Each container will have own mac and ip.
Problem:
========
->It connects directly to the network but it has different mac address, we can't access the container on machine without promiscuous mode.

modes:
i)bridge
ii)802.1q 

5.IPVlan:
modes:
i)L2 =>It connects to the system,so same mac address but ip address will connect to network directly, so different container will have different IPs.
cmd: network create:
sudo docker network create -d ipvlan --subnet 10.7.1.0./24 --gateway 10.7.1.3 -o parent=enp0s3 newsgard
cmd: container run: docker run -itd --rm --network newasgard --ip 10.7.1.92 --name <name>
ii)L3 : here the containers will connect to the host network but there is no layer 2, it's all layer 3. 
->The network will have its own ip.
->we will connect with the host like its a router.

->To reach the container, we have to go via the host.

cmd:docker network create -d ipvlan --subnet <ip> (no_gataway) -o parent=enp0s3 -o ipvlan_mode=l3 --subnet <ip> <name>
->when different container in different subnet share a same host, it can talk with eachother.

6.Overlay= it is a config which says which container can talk to which one.

7.None network(security):
it has no ip address, only loopback.
20/3/24:
web vs application server:
https://aws.amazon.com/compare/the-difference-between-web-server-and-application-server/
Web server:
==========
-> a server that hosts websites.
->Runs web server software like apache http, IIS(microsoft),...
=>Connected to internet
->simple page,static,protocol(ftp,http,smtp);
->doesn't support multi-threading.
=>It display the content as-if it is stored on the server.


Application server:
===================
->Does complex work,dynamic pages,protocol(ftp,http,smtp,rmi,rpc)
->supports multi-threading.
=>It has few template and dynamically loads data for the content.


web server:
https://developer.mozilla.org/en-US/docs/Learn/Common_questions/Web_mechanics/What_is_a_web_server

Nginx:
======
=>Install using sudo apt install nginx.
=>Two folder:
/etc/nginx/sites-enabled/default contains config file
/var/www/html contains default page(index.html)
=>on every config change, reload by systemctl nginx
=>On the same network, we can use ip of the server for checking.

1.A react app on nginx:
=======================

1.create a react app.
2.run "npm run build"
3.then copy the build content to another folder.(/var/www/html/app)
4.In the config file:
    ->write a location block with /static/
    location /static/ {
        alias <path to build static folder>
    }
    location /app{
        try_files $uri /index.html;
    }
5.reload nginx;
6.To check if nginx conf is successful:
sudo nginx -t;

7.A server block can listen for only one ip:port.

8.we can write as many conf file inside /etc/nginx/conf.d/ but it should follow 7th point here.

2. A vite app on nginx:

i)FROM node:latest as BUILD_IMAGE

WORKDIR /app/viteapp

COPY package.json .

RUN npm install

COPY . .

RUN npm run build 

#production image

FROM node:latest as PRODUCTION_IMAGE

WORKDIR /app/viteapp

COPY --from=BUILD_IMAGE /app/viteapp/dist/ /app/viteapp/dist/

EXPOSE 8080

COPY package.json .

COPY vite.config.js .

RUN npm install

CMD [ "npm","run","preview" ]

ii)remaining same as vite app.

express->dockerize

https://its-amit.medium.com/how-to-make-build-for-express-js-node-js-using-webpack-and-deployment-on-docker-9cd219ba24a2

http referer nginx:
if ($http_referer ~* "story") {
                        proxy_pass http://localhost:5173;
                        break;
}
if ($http_referer ~* "home"){
                        proxy_pass http://localhost:3001;
                        break;
}


26/3/24:
========
Task : Full deployment:
refer:./aFullEnvDep

1.VITE->Dockerize with nginx->Run via local nginx; src=vite4/
==================================================

=>Creation: npm create vite <app-name>;
=>A multi stage image with node build(part 1) and nginx deploy(part 2);
=>A conf.d folder and bind mount this to image

2.Backend(express)->Dockerize with nginx ->Run via local nginx: src=back2/
=============================================================

=>creation: index.js , express package install via npm, package.json

=>npm -g i webpack

=>npm  i webpack webpack-cli --save-dev

=>a webpack.config.file with configuration.

=>build:webpack in package.json script

=>npm run build

=>Multi step image with both as node.

=>run this dist with node ./dist/final.js

3.DB(mysql)->Dockerize->view the volumes:
=========================================

1.A sql file with data creation.
2.a dockerfile:
    ->mysql latest image
    ->env variable with mysql_root_password:root
    ->COPY ./stud.sql /docker-entrypoint-initdb.d/
    ->A new volume which mounts this /docker-entrypoint-initdb.d/

4.A overall config compose file with all these separate containers:
======================================================================

27/3/24:
===========

=>To get a free website:https://www.name.com/partner/github-students#:~:text=Free%20domain%20name,free%20domain%20names%20to%20students.
 

GCP:
=>Create a new vm with how to create:
https://cloud.google.com/compute/docs/instances/create-start-instance

setting ssh for the instance: so that I can access via the terminal:
=>

docker container run -it -v /tmp:/tmp -v /var/run/docker.sock:/var/run/docker.sock -v /var/lib/docker/containers:/var/lib/docker/containers:ro -e ACCOUNT_UUID={bc940e7b-1f0a-490d-981f-c3a920d02358} -e REPOSITORY_UUID={03c26cbc-7ee1-4f12-9955-2dd50a8f7e68} -e RUNNER_UUID={3923faa8-1568-5fb4-a09f-2bc7f74d06c9} -e RUNTIME_PREREQUISITES_ENABLED=true -e OAUTH_CLIENT_ID=BJsKUgntWzpMl68bqlbLQdMcIkTwnHTh -e OAUTH_CLIENT_SECRET=ATOAVu7kIMSk-Af_VDHLuXBepP7CA1lYDGPd5nA_bCgv7xi-7wW2DknNLNzqgD24IBC9B36ACFEB -e WORKING_DIRECTORY=/tmp --name runner-3923faa8-1568-5fb4-a09f-2bc7f74d06c9 docker-public.packages.atlassian.com/sox/atlassian/bitbucket-pipelines-runner:1

1.Certbot:
2.nginx config

FE-Small Tools:=Project.=1/4/24-2/4/24
===============
Description : Deployment of the staging branch on remote server for the front end development team with manual deploy of the build and written pipeline for it.

Server External IP: ssh root@13.201.173.55=>
    =>The ssh key of rishi is added to server.

1.Cloned the "git clone git@bitbucket.org:pmo-codingmart/qr_small_tools_fe.git" into qr_small_tools_fe.
2.Run npm install
3.npm run build
    =>For this to work, I have increased the previous swap of 2GB to now 4GB.
    =>In the package.json, in the scripts block, increase the "--max-old-space-size=1048" to "1548".
4.using the pm2,
    =>pm2 --name <appName> serve --spa <staticFolder> <port>
    =>using this deploy
5.In the nginx, create the conf.d file for this website:
    =>It is located at /etc/nginx/sites-enabled/quickrecruit.default
6.Check the bash file on /config/small-tools-fe-home.deploy
Check the website on https://staging.quickrecruit.com

notes:
======
    1.Request header to see if any errors.
    2.run bash script directly on server and use runner just for running not for build the application.
    3.use pm2 not docker because docker consumes lot of memory.
    4.since we used rm -rf ./quickrecruit on pipeline, during the running of the pipeline, the results will be 404 not found.
    6.how to increase swap size: https://www.baeldung.com/linux/increase-swap-space
    7.pm2 save to save all the instance in a server and pm2 restruct to up all the instance.

Things to study:
1.CDN, Cloudflare, AWS , prettier & pre-commit & husky

remote address:34.72.180.102:443

Things to study:
1.pm2,docker integration,jenkins, kubernetes.

3/4/24:
======

1.PM2:
=====

=>It is a open source process management for nodejs applications.

Install:
npm install pm2@latest -g 

start an app:
============
$ pm2 start app.js

$ pm2 start bashscript.sh
$ pm2 start python-app.py --watch
$ pm2 start binary-file -- --port 1520

# Specify an app name
--name <app_name>

# Watch and Restart app when files change
--watch

# Set memory threshold for app reload
--max-memory-restart <200MB>

# Specify log file
--log <log_path>

# Pass extra arguments to the script
-- arg1 arg2 arg3

# Delay between automatic restarts
--restart-delay <delay in ms>

# Prefix logs with time
--time

# Do not auto restart app
--no-autorestart

# Specify cron for forced restart
--cron <cron_pattern>

# Attach to application log
--no-daemon

Managing process:

$ pm2 restart app_name
$ pm2 reload app_name
$ pm2 stop app_name
$ pm2 delete app_name

List:
pm2 ls

log:
pm2 logs

dashboard: pm2 monit 

pm2 ecosystem:
this is generate an ecosystem.config.js file:

module.exports = {
  apps : [{
    name: "app",
    script: "./app.js",
    env: {
      NODE_ENV: "development",
    },
    env_production: {
      NODE_ENV: "production",
    }
  }, {
     name: 'worker',
     script: 'worker.js'
  }]
}

Starting an app:
1. we can use serve for first time.
2. we can start like this,
    =>pm2 start "npm run start"

To display log:
    =>$ pm2 start "npm run start" --attach

Passing arguments:
    =>$ pm2 start <startscript> -- arg1 arg2
    
    =>Any thing after -- is arguments.

configuration file:
==================

    =>we need configuration file when we need to manage multiple applications at the same time.

    =>create using pm2 ecosystem and start using 'pm2 start ecosystem.config.js'

    =>we will study more about it later.

Restarting:
=========

=>pm2 restart api= for single app
=>pm2 restart app1 app2 = for multiple app
=>pm2 restart all = for restarting all the app.

updating env variables:
========================
=>To update environment variables or PM2 options, specify the --update-env CLI option:

$ NODE_ENV=production pm2 restart web-interface --update-env

Stop:
=====

=>pm2 stop <app>, stop <process_id>, stop all, stop <app1> <app2>.

delete:
=======

=>pm2 delete app, pm2 delete all.

listing apps:
============

$pm2 ls

=>To specify the order of the list:
    =>$pm2 list --sort [name|id|pid|memory|cpu|status|uptime][:asc|desc]

Terminal dashboard:
=>$ pm2 monit gives you a simple way to monitor the resource usage of my app.

showing application metadata:
=>To display metadata about an application:
    $ pm2 show api
=>This will show where the error logs are located.

Restarting strategies:
=====================

=>Pm2 automatically restarts in case of app crashes.

cron restart: 
=============
    =>It is the periodic restart of the application without manual intervention.
    $pm2 start app.js --cron-restart="0 0 * * *"
        =>the syntax meaning is cron-restart=consists of five fields representing minute, hour, day of month, month, and day of week.
        =>Here it restarts daily at 12:00AM 

Restart on file change: just add --watch on start application.
-=-=-=-=-=-=-=-=-=-=-=
    =>It has watch for specific file,watch_delay and ignore_watch and these can configured ecosystem file.

Memory based restart:$ pm2 start api.js --max-memory-restart 300M
======================

Restart Delay:
=============
$ pm2 start app.js --restart-delay=3000

No auto restart:
=================
$ pm2 start app.js --no-autorestart

Skip Auto Restart For Specific Exit Codes
==========================================
$ pm2 start app.js --stop-exit-codes 0

Exponential backoff restart:
=============================
$ pm2 start app.js --exp-backoff-restart-delay=100
    =>It exponentially increases the restart time.

Logs:
=====
pm2 logs <app name or id>


Flusing logs:
============

pm2 flush <appname> =>will clear all the log data.

Auto prefixing log with time:
    =>pm2 start appname --time

Persistent application:
======================
    =>This is used if we want our application to run even after system restart.

    1.$pm2 save => It will save all the running apps in pm2.

    2.$pm2 resurrect => It will start all the previously saved application.

    3.$pm2 startup => This is automatic method, once it is given only one time, the saved pm2 apps will start on every restart and pm2 also starts. 
        =>Follow the steps by executing this command.

    4.$pm2 unstartup => To remove the startup.

Configuration file:
==================

    =>when managing multiple application with pm2, we a js config file known as ecosystem.config.js => we can start the application with this.

    =>To create a sample configuration file:
        pm2 init simple

    => Then we can manage multiple apps in the same way as normal.
        #start all the applications
        pm2 start ecosystem.config.js

        # Stop all
        pm2 stop ecosystem.config.js

        # Restart all
        pm2 restart ecosystem.config.js

        # Reload all
        pm2 reload ecosystem.config.js

        # Delete all
        pm2 delete ecosystem.config.js

    =>There are various configuration attributes located at:https://pm2.keymetrics.io/docs/usage/application-declaration/

Cluster mode:
=============
=>A cluster will use a single core.
=>So 8 core cpu can have 8 cluster on max but we can increase the cluster but it will share the available cores, it is bad for performance
module.exports = {
  apps: [{
    name: "vite-app",
    script: "npm",
    args: "run dev", // Assuming your start script for Vite is 'npm run dev'
    cwd: "/home/rishikeshb/Documents/DevOps/pm2/ex1", // Change this to the actual path of your Vite app
    watch: true,
    ignore_watch: ["node_modules"],
    instances: "4",
    exec_mode: "cluster"
  }]
};

4/4/24:
======
Deployment of kalanju backend on v4-accounts-development branch:
=================================================================
    Repo : https://bitbucket.org/pmo-codingmart/kalanju-new-be/src/v4-accounts-development/bitbucket-pipelines.yml

    ->source branch: accounts-dev
    url:alpha.kalanju.com 
        ->test: alpha.kalanju.com/api/accounts-service/healthcheck.
    ->server ssh root@alpha.kalanju.com

1. Created a new branch : v4-accounts-development from accounts-dev

2. Wrote a new pipeline in bitbucket

3. wrote a deploy(script) file:
    #!/bin/bash
    cd /root/account-service
    rm -rf node_modules/
    npm i
    npx prisma generate
    cp /root/config/env/account-service/.env /root/account-service/
    /usr/bin/pm2 restart account-service 

        error:Error: ENOENT: no such file or directory, open '/root/account-service/node_modules/.bin/prisma schema build_log.wasm'
        =>When running, the env is not available error entenv error because the node version on server is 16 but we copied from the node modules from 18.

        =>delete the node modules and install using npm i for the node 16 then generate npx prisma generate there.

4. Create env -> account-service -> .env = It is copied from the kiruthiga akka's env.

5. Then copied this env to the account-service on home.

6.we can check this from : https://alpha.kalanju.com/api/accounts-service/healthcheck

CDN(cloudflare) and (cloudfront) to AWS bucket:
==============================
1.cloudfront - youtube video: https://www.youtube.com/watch?v=kbI7kRWAU-w&ab_channel=SamMeech-Ward

2.cloudflare - youtube video: https://www.youtube.com/watch?v=KnlB52S9P3Y&ab_channel=FooBarServerless

To access a bucket: https://<bucket_name>.s3.<region>.amazonaws.com/<filepath>/<filename>

Eg:https://DOC-EXAMPLE-BUCKET.s3.us-west-2.amazonaws.com/photos/puppy.jpg

problem : The loading of quick recruit website takes so much time, in order to reduce the loading time, we are gonna use cdn of cloud flare.

Tools to check performance: https://tools.keycdn.com/performance
==========================

steps:
1.set up distribution:

    => search cloud front in aws services.
    =>create a new distribution.
    =>In origin domain, select the bucket you want to use for cdn.
    =>Bucket Access - yes use OAI.
        ->This allows us to keep the s3 bucket private and only allows through cloud front.
    =>origin access identity = create new oai.
    =>Bucket policy => yes, update the bucket policy.
    =>no custom header, no origin shield.
    =>In default cache behaviour, redirect http to https.
    =>In allowed methods, get , head only.
    =>No restrict viewer access.
    =>In cache policy, use caching optimized.
    =>In location, all locations.
    =>Don't need Custom domain name.
    =>Create distribution will take few mins.

2.There will be a distribution domain name for the this.

3.to get the file from cloudfront:
    ->https://<distribution_name>/<s3_bucket_name>
    ->we can see this image.

4. How to use this in code:
    ->when reading, use <distribution_name>+<object_name>
    =>Now everything will come from cloudfront.

cloudflare static hosting:
==========================

Refer:https://developers.cloudflare.com/pages/framework-guides/deploy-anything/
https://developers.cloudflare.com/pages/framework-guides/deploy-a-react-site/

problem: 
    =>We can deploy the static site of vite mcq on cloudflare but it gives only .pages.dev (website);
    =>we can run a backend server with a domain which runs on http;
    =>so we need a backend domain to convert it to https;
    =>then we can use .pages.dev for frontend and server on test.codingmart.com which is not allowed by the upper management.

    =>They are saying the will check and say.

kubernetes:=9/4/24
===========

=>Kubernetes is a container orchestration tool.

=>Suppose we have a thousands of containers, if we have to push a update, then we have to update every container manually which is tedious. so we can use kubernetes to automate these processes.

=>Features:
    ->Service discovery and load balancing.
    ->Storage orchestration.
    ->Automated rollouts and rollbacks.
    ->Automatic bin packing.
    ->Self-healing.
    ->Secret and configuration management.
    ->Batch execution.
    ->Horizontal scaling.
    ->IPv4/IPv6 dual-stack.
    ->Designed for extensibility.

1.Installed kubectl:https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management

2.Installed minikube:https://minikube.sigs.k8s.io/docs/start/

3.Installed kind: https://kind.sigs.k8s.io/docs/user/quick-start/ by homebrew.



Kubernetes - Basics terms:
    In Kubernetes, objects are entities used to define the desired state of the cluster. These objects represent various components of an application or the Kubernetes system itself. Kubernetes uses these objects to manage and maintain the state of the cluster, ensuring that the applications run as intended. Some common Kubernetes objects include:

    1. **Pods**: Pods are the smallest deployable units in Kubernetes. They represent a single instance of a running container or multiple tightly coupled containers that share resources, such as storage and network, and are scheduled together on the same node.

    2. **ReplicaSets**: ReplicaSets ensure that a specified number of Pod replicas are running at any given time. They help in scaling the number of identical Pods up or down based on the desired state.

    3. **Deployments**: Deployments provide declarative updates to Pods and ReplicaSets. They manage the rollout and rollback of changes to applications by updating the ReplicaSets with new Pod specifications.

    4. **Services**: Services define a logical set of Pods and a policy by which to access them. They provide a consistent way to access applications running in Kubernetes, abstracting away the details of individual Pod IP addresses.

    5. **Volumes**: Volumes are used to persist data in Kubernetes. They allow containers within Pods to share data or persist data beyond the lifecycle of a Pod.

    6. **ConfigMaps and Secrets**: ConfigMaps and Secrets are used to decouple configuration artifacts from container images. They store configuration data and sensitive information, respectively, which can be injected into Pods as environment variables or mounted as files.

    7. **Namespaces**: Namespaces provide a way to divide cluster resources into virtual clusters, making it easier to manage and isolate workloads within a single physical cluster.

    8. **DaemonSets**: DaemonSets ensure that a copy of a Pod runs on all or some of the nodes in the cluster. They are typically used for system-level services such as monitoring agents or logging collectors.

    9. **StatefulSets**: StatefulSets manage the deployment and scaling of stateful applications, such as databases. They provide stable, unique network identities and persistent storage for each Pod.

    10. **Jobs and CronJobs**: Jobs and CronJobs are used to run batch or scheduled tasks in the cluster. Jobs are used for one-off or finite tasks, while CronJobs are used to run tasks at specified intervals.

    These are just some of the core Kubernetes objects. Kubernetes offers a wide range of objects to manage various aspects of containerized applications and infrastructure. Understanding these objects and how to use them effectively is essential for managing applications in Kubernetes.

steps:
=>Installed kubectl.
=>Installed minikube.

3.Start minikube: minikube start.

4.To monitor: minikube dashboard.

5. TO see services: kubectl get services

6. To create a deployment: minikube kubectl -- create deployment hello-minikube --image=kicbase/echo-server:1.0

7. To exposing the deployment:
minikube kubectl -- expose deployment hello-minikube --type=NodePort --port=8080

=========================================
Freecode camp video:https://www.youtube.com/watch?v=kTp5xUtcalw&ab_channel=freeCodeCamp.org
====================

Microservices architecture:
===========================

=> A variant of the service-oriented architecture structural style - arranges an application as a collection of loosely coupled services.

=> In a microservices architecture, services are fine-grained and the protocols are lightweight.

=>In microservices, we separate each service into single one and deployed independently.

=>Loosely coupled.

=>Each microservice has it's own db.

Monolithic architecture:
=======================

=>Built as a single unit.
=>Deployed as a single unit.
=>Duplicated on each server.
Ex: 3 -tier apps.

Cloud native:
============
Definition:

=> In summary, "cloud-native" refers to a modern approach to building and running applications that harnesses the power of cloud computing. Key aspects of cloud-native applications include microservices architecture, containerization, dynamic orchestration, elasticity, resilience, DevOps practices, observability, and utilization of cloud-native services. This approach enables organizations to innovate rapidly, scale efficiently, and deliver high-quality applications while leveraging the full potential of cloud computing resources.

Concepts of cloud native:
==========================

    =>Speed and agility,no downtime, fast added features.

    Trail map:

    =>containerization, CI/CD, Orchestration(kubernetes), observability(prometheus),Service proxy mesh, security and policy.

Orchestrator:
=============

    =>Manage:    
        ->infrastructure
        ->containers
        ->Deployment
        ->scaling
        ->Failover
        ->Health monitoring
        ->App upgrades, zero-downtime deployments 
    =>Install your own:
        ->Kubernetes, Swarm, Service Fabric 
    =>Orchestrators as a service:
        ->AKS, EKS, GKS.

35:11 = video 

10/4/24=Freecodecamp = docker containers and kubernetes fundamentals:
===============================================================

=>Enable kubernetes on docker desktop.

=>use yamllint.com to check the yaml file validity.

Docker compose v1 vs v2:
=======================
=>we can create multiple instance of the same compose app.

v2: docker compose =>install =>sudo apt-get compose-plugin.

=>we can set resources for the containers,
    ->services:
        redis:
          image: redis:alpine
          deploy:
            resources:
              limits:
                cpus:'0.50'
                memory: 150M
              reservations:
                cpus: '0.25'
                memory: 20M 

Kubernetes/k8s:
===============
cluster>namespace>pods
=>Leading container orchestration tool.
=>Designed as a loosely coupled collection of components centered around deploying, maintaining and scaling workloads.
=>Vendor-neutral
    ->Runs on all cloud providers.

Architecture:
->Master node/ control panel.(kubernetes is running here.)
->Worker node (where the projects are deployed.)

Flow:
    =>Cluster > Node > Pod > Container.
    
Running kubernetes locally:
===========================

    =>Local k8s:
    ===========
        ->Requires virtualization:
            ->Docker desktop
            ->MicroK8s 
            ->Minikube (Kubernetes IN Docker)
        ->Runs over Docker Desktop:
            ->Kind.
        
        =>Docker desktop only one node but others multiple nodes.

    => we can run kubernetes in three ways:
        1.Docker desktop.
            ->Enable kubernetes on app and maybe reset kubernetes cluster.
        2.minikube.
            ->Start: minikube start
            chk: kubectl cluster-info
        3.Kind.=> installed kind using brew.
            ->Kind create cluster.
            ->Kind delete cluster.
k8s cli:
========
    =>K8s api server runs on master node and it exposes a rest api - a only way to communicate with the clusters.

    =>Kubectl configuration stored locally on ${Home}/.kube/config.

k8s context:
============
    =>A context is a group of access parameters to a k8s cluster.

    =>Contains a kubernetes cluster, a user and a namespace.

    =>The current context is the cluster that is currently the default for kubectl.

    command cheat sheet:
    1.kubectl config current-context = get the current context.
    2.kubectl config get-contexts = list all context.
    3.kubectl config use-context <contextName> = set the current context.
    4.kubectl config delete-context <contextName> = delete a context from the config file.
    5.kubectl config rename-context <oldName> <newName> = to change name.
    6.kubectx <contextName> = will quickly change the context. 

ways to create resources in kubernetes:
=======================================
    1.Imperative:
        =>Using kubectl commands, issue a series of commands to create resources.
        =>Great for learning, testing and troubleshooting.
        =>It's like code.
        Ex:
            $ kubectl run mynginx --image=nginx --port=80
            $ kubectl create deploy mynginx --image=nginx --port=80 --replicas=3
            $ kubectl create service nodeport myservice --targetPort=8080
            $ kubectl delete pod nginx
    2.Declarative:
        =>Using kubectl and yaml manifests defining the resources that you need.
        =>Reproducible, repeatable.
        =>Can be saved in source control.
        =>It's like data that can be parsed and modified.
        Ex: YAML File (deploy.yaml)
            apiVersion: v1 
            kind: pod 
            metadata:
                name: myapp pod
                labels:
                  app: myapp
                  type: front-end 
                spec:
Deploying a docker image from hub with kubernetes:
=================================================

    =>Start the minikube.

    =>kubectl create deployment hello-minikube --image=<imagename>

    =>kubectl expose deployment hello-minikube --type=NodePort --port=8080

    =>port forward:
        ->kubectl port-forward service/hello-minikube 7080:8080

Deploying a docker image from local with kubernetes:
===================================================

    1.create a file like this bb.yaml:
        apiVersion: apps/v1
        kind: Deployment
        metadata:
        name: bb-demo
        namespace: default
        spec:
        replicas: 1
        selector:
            matchLabels:
                bb: web
        template:
            metadata:
                labels:
                    bb: web
            spec:
                containers:
                    - name: bb-site
                    image: getting-started
                    imagePullPolicy: Never
        ---
        apiVersion: v1
        kind: Service
        metadata:
        name: bb-entrypoint
        namespace: default
        spec:
        type: NodePort
        selector:
            bb: web
        ports:
            - port: 3000
                targetPort: 3000
                nodePort: 30001

    2.kubectl apply -f bb.yaml

    3.kubectl get deployments || get pods || get services

    4.Check on localhost:30001

    5.kubectl delete -f bb.yaml => to delete all the things from apply.

15/4/24:
========

Certbot: https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-20-04


Kubernetes - Master and worker nodes:
=====================================

1. setting up private repository for docker images :https://www.digitalocean.com/community/tutorials/how-to-set-up-a-private-docker-registry-on-ubuntu-20-04


2.Running an image from private registry on minikube:
======================================================

    =>Create a deployment:  
        kubectl create deployment samreact --image=rishicollinz.me/samreact

    =>See a file for spec.template.spec.containers and note down the container name:
        kubectl get deployment samreact -o yaml --kubeconfig ~/.kube/config
    
    =>Set the server address, for the deployment:
        kubectl set image deployment/samreact samreact=rishicollinz.me/samreact --kubeconfig ~/.kube/config --namespace=default

    =>create a kubernetes secret with registry username and password:
        kubectl create secret docker-registry regcred --docker-server=rishicollinz.me --docker-username=rishicollinz --docker-password=wantedbyme

    =>Add the secret to deployment:
        kubectl patch deployment samreact -p '{"spec":{"template":{"spec":{"imagePullSecrets":[{"name":"regcred"}]}}}}' --kubeconfig ~/.kube/config --namespace=default
    
    =>Expose the deployment:
        kubectl expose deployment samreact --type=NodePort --port=3000

    =>Port forward it:
        kubectl port-forward service/samreact 3000:3000

16/4/24:
========
k3s vs k8s:
===========

=>K3S is distribution of kubernetes and the daemon are the same but packages it differently.

features:
=========
    =>It is lightweight, single binary.
    =>Low resource usage.
    =>Compatibility
    =>Ethos.

Changes from k8s :
==================
    =>Combined all processes to single binary.
    =>Cross-compiled for arm.
    =>Dropped non-csi storage providers.(third-party storage providers are dropped).
    =>Dropped cloud providers Integration.
    =>Automatic certificate creation/rotation.
    =>Bundle all user-space tools too (iptables, du, find, etc);

K3S added features:
===================
    =>Automatic manifests deployment = (If there is a yaml file in a project, then during the installation of the k3s, it is automatically deployed)

    =>KINE - for using sqlite/dqlite/MYSQL/postgresQL instead of etcd for storage.

    =>Custom helm chart crd.

    =>CoreDNS, Metrics and Traefik by default.

Installing k3s:
================
    =>curl -sfL https://get.k3s.io | sh

How it is better than full kubernetes:
=====================================
    =>small binary, low resources, single master/node is possible, easy install/quick cluster launch.
    =>Bundled dependencies.

Application:
============
    =>Development environments, Test environments, Experiments, Learning.

Namespaces:
============
    =>It is a kubernetes resource that allow you to group multiple resources.
        E.g: Dev, Test, Prod.
    =>K8S creates a default namespace named default.
    =>Objects in one namespace can access objects in different one.
    =>Deleting a namespace will delete all its child objects.
    
    =>To list all the namespace: kubectl get ns

    Defining a namespace:
    ====================
    ->Namespace Definition:
        =>You define a namespace.
            yaml:
                apiVersion: v1
                kind: Namespace 
                metadata:
                    name: Prod 
    ->Pod Definition:
        =>You specify the namespace when defining objects.
            Pod definition - yaml:
                apiVersion: v1 
                kind: pod 
                metadata:
                    name:myapp-pod
                    namespace: prod 
                spec:
                    containers:
                    - name: nginx-container
                      image: nginx 

    ->NetworkPolicy Definition:
        apiVersion: networking.k8s.io/v1
        kind: NetworkPolicy
        metadata:
            namespace: clientb
            name: deny from other Namespaces
        spec:
            podSelector:
                matchLabels:
            ingress:
            - from:
                - podSelector: {}
    
    ->ResourceQuota definition:
        apiVersion: v1
        kind: ResourceQuota
        metadata:
            name: compute-quota 
            namespace: prod 
        spec:
            hard:
                pods: "10"
                limits.cpu: "5"
                limites.memory: 10Gi

    Command cheat sheet:
    ===================
        =>kubectl get namespace / kubectl get ns = for getting all namespaces.
        
        change namespace:
        =================
        =>kubectl config set-context --current --namespace=[namespaceName] = set the current context to use a namespace. (to change namespace);

        =>kubectl create ns <namespaceName> = to create a namespace.

        =>kubectl delete ns <namespaceName> = to delete a namespace.

        =>kubectl get pods --all-namespaces = list all pods in all namespaces.

        To get pods from a specific namespace:
        =====================================
        =>kubectl get pods --namespace=kube-system || kubectl get pods -n kube-system 

Nodes:
=====
Definition:
    =>Nodes are physical or virtual machines. Together, they form a cluster.
Master node:
===========
    =>The master node is also known as control plane.
    control panel contains:
        =>Kube-controller-manager.
        =>cloud-controller-manager.
        =>kube-apiserver=>etcd(key value datastore for cluster state data)
        =>kube-scheduler.

    Kube apiserver:
    ===============
        =>It exposes an REST interface.(using which all the kube cli tools interact)
        =>save state to the datastore(etcd);
        =>All clients interact with it, never directly to the datastore.

    etcd:
    ====
        => Act as the cluster datastore for storing state.
        => Key-value store.
        => Not a database or a datastore for applications to use.
    
    Kube-control-manager:
    =====================
        =>The controller of controllers!
        =>It runs controllers,
            =>Node controller.
            =>Replication controller.
            =>Endpoints controller.
            =>Service account & Token controllers.
    
    cloud-control-manager:
    =======================
        =>Interact with the cloud providers controllers,
            =>Node:
                ->For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding.
            
            =>Route:
                ->For setting up routes in the underlying cloud infrastructure.
            
            =>Service:
                ->For creating, updating and deleting cloud provider load balancers.
            
            =>Volume:
                ->For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes.
    
    kube-scheduler:
        =>watches newly created pods that have no node assigned, and selects a node for them to run on.

        =>Factors:
            ->Individual and collective resource requirements.
            ->Hardware/software/policy constraints.
            =>Affinity and anti-affinity specifications.
            =>Data locality.
    Addons:
        =>DNS, WebUI(dashboard), cluster level logging, container resource monitoring.

Worker node:
============
    => Nodes running the containers is called worker node.

    => when a worker node is added to the cluster, it automatically installs,
        ->Kube-proxy
        ->kubelet
        ->container runtime
        These are the services necessary to run pods, and they are managed by master node.
    
    kubelet:
        =>Manage the pods lifecycle.
        =>Ensures that the containers described in the pod specs are running and healthy.

    kube-proxy:
        =>A network proxy that manages network rules on nodes . All network traffic goes through it.
    
    container runtime:
        => k8s supports several container runtimes.
        =>Must implement the kubernetes container runtime interface:
            ->Moby
            ->containerd 
            ->cri-O
            ->Rkt 
            ->Kata 
            ->Virtlet

Nodes pool:
==========
    =>A node pool is a group of virtual machines, all with same size.
    =>A cluster can have multiple node pools,
        =>These pools can host different sizes of VMs.
        =>Each pool can be autoscaled independently from the other pools.

Microk8s:
 =>Install using snap from official website.
 =>For dashboard: microk8s dashboard-proxy.

Master and slaves:
==================
Creation:
=========
on master , microk8s add-node
on another vm(worker), use the cmd from master (join cmd).


Deletion:
on slave, microk8s leave.
on master and remaining nodes, microk8s remove-node name

Label as master and slave:

cmd:kubectl label node <nodeName> node-role.kubernetes.io/master=master

To run a docker container:
=========================
    busybox:
    =>kubectl run mykubes -it --image busybox -- sh
    nginx:
    =>kubectl run nginx --image nginx 

    To make a thing run on a specific node:
    ======================================
        apiVersion: apps/v1
        kind: Deployment
        metadata:
        name: mynginx
        spec:
        replicas: 1
        selector:
            matchLabels:
            app: nginx
        template:
            metadata:
            labels:
                app: nginx
            spec:
            containers:
            - name: nginx
                image: nginx
            nodeSelector:
                kubernetes.io/hostname: ubuntu22lts

k8s - replicas: 
kubectl create deployment mynginx --image=nginx
kubectl scale deployment mynginx --replicas=2

or we can use spec->replicas in yaml service file.

Things to study:

    k8s = Master and slaves, Job, volumes,

    k8s = namespace - yaml file, service,clusterIp, nodeport, services, max-surge, replicaset, rolling update, pod health check
refer: https://chat.openai.com/share/1233843c-26dd-4011-af17-ef45999ffc25

17/4/24:
Jenkins:
========
refer: https://www.youtube.com/watch?v=6YZvp2GwT0A&ab_channel=DevOpsJourney
1.Install java on vm and host :  sudo apt install openjdk-21-jdk

Introduction:
    =>It is a automation platform, that helps to automate build,test,deploy the project.

    =>To achieve CI/CD.= key feature.

    =>Jenkins will build,test and deploy.

    =>It uses plugin to extend its functionality.

    =>Jenkins pipeline uses groovy language.

Jenkins infrastructure:
========================
    1. Master server:
        ->Controls pipelines
        ->Schedules Builds 
    2. Agents/Minions
        ->Perform the Build.

workflow: A developer pushes code to github, jenkins master will know there is a commit to repo, so schedules a build for a agent and the agent will build and test and deploy.

Agent types:
===========
    ->Permanent Agents:
        ->Dedicated server for running jobs.
            Requirements:
                ->Java.
                ->Build tools (if needed)
                ->ssh for connection.
    
    ->Cloud Agents:
        ->Ephemeral/Dynamic agents spun up on demand.
            Eg: docker, kubernetes, aws

Build Types:
============
    ->Freestyle Build:
    ==================
        ->simplest method to create a build.
        ->Feels like shell scripting

    ->Pipelines:
        ->Use the groovy syntax
        ->pipelines are commonly broke down to different stages.
            Eg:
            stages  
                stage('clone'){

                }
                stage('build'){

                }
                stage('test'){

                }
                stage('package'){

                }
                stage('deploy'){

                }

Jenkins web GUI walkthrough:
    =>Dashboard=>manage nodes and clouds - this is where we add the agents and cloud .
    =>Manage credential for storing api keys, ssh keys.

22/4/24:
=======

1.creating a simple freestyle job:
==================================

    =>Create a freestyle job. and explored the config.

2.Workspace:
=>In configure, in shell, create a test file and check in workspace
workspace located: /var/lib/jenkins/workspace

Setting up master and worker nodes on jenkins:
==============================================

1. Install jenkins on master node (a vm = u24) and java.
2. Install java on worker node.

3.Create a node on jenkins dashboard, with a remote directory, unique label and use this node only for label specified.
4. On dashboard > manage jenkins > security > tcp = select random.
5. Run the 3 commands with secret file on remote agent(ubuntu24vm).
6. Now the master is connected with worker = check on nodes in dashboard.

7.Create a new job and select a git repo with shell command "python3 first.py"
8.On config of job, select restrict on label only node and type the nodelabel name.
9.Now build and check on the console to see if it run on worker node.

Trigger a build of jenkins using github webhook:
================================================
1. Jenkins job:
    =>Make sure the git hub repo is added to job.
    =>In jenkins job, change the build trigger to "GitHub hook trigger for GITScm polling" and select apply.

2. Creating a webhook in github:

    =>In repo settings, in webhooks,
        =>In payload url: http://<jenkins dashboard IP:port>/github-webhook/
        e.g: https://3694-2401-4900-6056-d797-9ee1-cf4e-b550-b140.ngrok-free.app/github-webhook/
        here , the ip of jenkins dashboard is internal, so we use ngrok to create a global ip:
            =>install ngrok : sudo snap install ngrok.

            =>Go to dashboard of ngrok on web and get this cmd : ngrok config add-authtoken 2fSSkV4cZ6Vvh9PRNHrypabVtrq_LypxvXr9odmHPGvakGsP
            =>run it on terminal.

            ngrok http http://localhost:8080
            =>for me ngrok didn't work on vm, so I port forwarded and used it on host.

            =>Now add the ngrok forward ip to github webhook.

3. Now any push to the repo will result in building of the same

A jenkins job's build triggers the script:
=========================================
1. Have a .deploy file on root of the project.
2. file content:
    #!/bin/bash
    echo "deploy file started"
    export PATH=$PATH:/home/ubuntu/.nvm/versions/node/v20.12.2/bin/
    cd /home/ubuntu/jenkins/workspace/Workerjob/viteC
    rm -rf node_modules/
    /home/ubuntu/.nvm/versions/node/v20.12.2/bin/npm i
    /home/ubuntu/.nvm/versions/node/v20.12.2/bin/npm run build
    #/home/ubuntu/.nvm/versions/node/v20.12.2/bin/npm install vite --save-dev
    rm -rf dockerfile index.html node_modules package-lock.json package.json public/ src/ vite.config.js 
    /home/ubuntu/.nvm/versions/node/v20.12.2/bin/npm i express
    /home/ubuntu/.nvm/versions/node/v20.12.2/bin/pm2 delete ecosystem.config.js
    /home/ubuntu/.nvm/versions/node/v20.12.2/bin/pm2 start ecosystem.config.js
    echo "bash file completed"

Build step shell cmd:
chmod +x vite.deploy
echo $PATH
./vite.deploy

23/4/24:
=======
Things to do:
=============
1. Running the working node on gcp.(done);
2. jenkins using docker.
3. pipeline and groovy.
5. deploy using bash but if curl req not 200 then revert the build to previous success and send email.
6. docker cloud worker and master setup.
7. health check on kubernetes for 5th step.
8. study kubernetes.
9. Data engineer.

1.Running the gcp server as worker node:
========================================
=>refer ssh way : https://devopscube.com/setup-slaves-on-jenkins-2/

1. create ssh key generation on gcp server.
2. add the public key to the authorized keys of the server.
3. add the private key to the global credential of the jenkins.
4. use the credential for the setup of the node.


2. Health check:
================
    =>Ways to get only the http request status code:
        1. curl -s -o /dev/null -w "%{http_code}" http://www.example.org/
        2. curl -I http://localhost:3000/ | head -n 1|cut -d$' ' -f2

Migrating AWS services from one account to another:
====================================================
refer: https://aws.amazon.com/blogs/architecture/migrate-resources-between-aws-accounts/

1. Ec2 eks ebs vpc region maranum.